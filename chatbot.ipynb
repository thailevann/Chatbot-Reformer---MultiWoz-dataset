{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvyhGQvukezK"
      },
      "source": [
        "#**Set up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIVcFtgbkcIH",
        "outputId": "8a800aaf-5fd5-4b9a-c00c-3762c9574bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trax==1.4.1\n",
            "  Using cached trax-1.4.1-py2.py3-none-any.whl (637 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (1.22.4)\n",
            "Collecting funcsigs\n",
            "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (4.8.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (5.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (1.16.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (0.4.6+cuda11.cudnn86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (3.7.1)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (0.4.6)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (0.25.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from trax==1.4.1) (1.4.0)\n",
            "Collecting tensorflow-text\n",
            "  Using cached tensorflow_text-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym->trax==1.4.1) (6.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym->trax==1.4.1) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym->trax==1.4.1) (0.0.8)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax->trax==1.4.1) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (4.39.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (5.12.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->trax==1.4.1) (1.4.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (1.15.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (1.12.0)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (8.1.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (2.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (2.27.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (0.1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->trax==1.4.1) (4.65.0)\n",
            "Collecting tensorflow<2.13,>=2.12.0\n",
            "  Using cached tensorflow-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text->trax==1.4.1) (0.13.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax==1.4.1) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax==1.4.1) (3.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.4.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.4.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.4.1) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.4.1) (1.26.15)\n",
            "Collecting tensorboard<2.13,>=2.12\n",
            "  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (23.3.3)\n",
            "Collecting protobuf>=3.12.2\n",
            "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 KB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (3.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (0.2.0)\n",
            "Collecting keras<2.13,>=2.12.0\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (16.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (67.6.1)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 KB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (0.31.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (1.51.3)\n",
            "Collecting wrapt\n",
            "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.4.1) (1.59.0)\n",
            "Collecting protobuf>=3.12.2\n",
            "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (2.16.3)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax==1.4.1) (3.2.2)\n",
            "Installing collected packages: funcsigs, wrapt, tensorflow-estimator, tensorboard-data-server, protobuf, keras, tensorboard, tensorflow, tensorflow-text, trax\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.11.2\n",
            "    Uninstalling tensorboard-2.11.2:\n",
            "      Successfully uninstalled tensorboard-2.11.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "Successfully installed funcsigs-1.0.2 keras-2.12.0 protobuf-3.20.3 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-text-2.12.0 trax-1.4.1 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install trax==1.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3ve7zFkkjD",
        "outputId": "d487d82f-981b-4b22-d387-d070abad10f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trax                          1.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep trax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simple-colors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsVIrfbwqzv1",
        "outputId": "c161e753-170b-4042-f5bb-6b7f352911a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: simple-colors in /usr/local/lib/python3.9/dist-packages (0.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD1JQYNKkmc4"
      },
      "source": [
        "#**Import library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpj8QdSXkO0q",
        "outputId": "114a471c-c4a4-4eee-c91d-4e98ffdfc73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rd_MfdXpMM3"
      },
      "source": [
        "#**Load data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td4KWobz_JTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3dd733-8817-4c06-f30c-178fbe7269fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "DATA_FILE = 'data.json'\n",
        "DATA_DIR = 'drive/MyDrive/52000820 - Deep learning/Cuoi Ki NLP/data'\n",
        "DIALOGUE_DB = {}\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "VOCAB_DIR = 'drive/MyDrive/52000820 - Deep learning/Cuoi Ki NLP/data/vocabs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7e0pqGD_Orv"
      },
      "outputs": [],
      "source": [
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file:\n",
        "        db = json.load(file)\n",
        "    return db\n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByzQkpJDbjhy",
        "outputId": "dab1d583-d765-4a0a-8eda-a1ae1268998f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số cuộc hội thoại: 10438\n"
          ]
        }
      ],
      "source": [
        "print(f'Số cuộc hội thoại: {len(DIALOGUE_DB)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Những người có hộp thoại đa miền (nhiều lĩnh vực) có \"MUL\" trong tên tệp của họ trong khi hộp thoại một miền có \"SNG\" hoặc \"WOZ\"."
      ],
      "metadata": {
        "id": "XitY-J0ZnueE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To2Io_Nfbuis",
        "outputId": "e6ded968-80d7-4bbe-b343-d193e0a8d5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SNG01856.json', 'SNG0129.json', 'MUL2168.json', 'SNG01445.json', 'MUL2105.json', 'PMUL1690.json', 'MUL2395.json']\n"
          ]
        }
      ],
      "source": [
        "# print 7 keys from the dataset to see the filenames\n",
        "print(list(DIALOGUE_DB.keys())[0:7])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Có 10.438 cuộc hội thoại, mỗi cuộc hội thoại nằm trong một tệp riêng.\n",
        "\n",
        "Mỗi tệp cũng được tải vào một từ điển và mỗi tệp có ba khóa như sau:"
      ],
      "metadata": {
        "id": "UTSLjui9oY4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMP5L0Bmb4P9",
        "outputId": "c8bd67b8-7066-439e-a5c0-66b87e5f09a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['new_goal', 'goal', 'log'])\n"
          ]
        }
      ],
      "source": [
        "# get keys of the fifth file in the list above\n",
        "print(DIALOGUE_DB['SNG0073.json'].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "goal: chứa một số khóa liên quan đến mục tiêu của cuộc hội thoại"
      ],
      "metadata": {
        "id": "v-UpWpeyopIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkRRzg5Rcgln",
        "outputId": "50beb8f0-e330-4b15-a10a-c71fb4292ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'taxi': {'info': {'leaveAt': '17:15',\n",
              "   'destination': 'pizza hut fen ditton',\n",
              "   'departure': \"saint john's college\"},\n",
              "  'reqt': ['car type', 'phone'],\n",
              "  'fail_info': {}},\n",
              " 'police': {},\n",
              " 'hospital': {},\n",
              " 'hotel': {},\n",
              " 'attraction': {},\n",
              " 'train': {},\n",
              " 'message': [\"You want to book a <span class='emphasis'>taxi</span>. The taxi should go to <span class='emphasis'>pizza hut fen ditton</span> and should depart from <span class='emphasis'>saint john's college</span>\",\n",
              "  \"The taxi should <span class='emphasis'>leave after 17:15</span>\",\n",
              "  \"Make sure you get <span class='emphasis'>car type</span> and <span class='emphasis'>contact number</span>\"],\n",
              " 'restaurant': {}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "DIALOGUE_DB['SNG0073.json']['goal']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "log: Nó là một danh sách các từ điển và mỗi thành phần của danh sách này cũng chứa một số mô tả."
      ],
      "metadata": {
        "id": "1_L3JtG0oua1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfjDHe1rcyup",
        "outputId": "0100f846-ac0e-48e5-a64f-21bc317999be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \"I would like a taxi from Saint John 's college to Pizza Hut Fen Ditton .\",\n",
              " 'metadata': {},\n",
              " 'dialog_act': {'Taxi-Inform': [['Depart', 'saint johns college']]},\n",
              " 'span_info': [['Taxi-Inform', 'Dest', 'pizza hut fen ditton', 11, 14],\n",
              "  ['Taxi-Inform', 'Depart', \"saint john 's college\", 6, 9]],\n",
              " 'turn_id': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# get first element of the log list\n",
        "DIALOGUE_DB['SNG0073.json']['log'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSKOVsbSdYoZ",
        "outputId": "c6d9dd8d-7b4a-487a-f202-7311b79917d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Person 1:  I would like a taxi from Saint John 's college to Pizza Hut Fen Ditton .\n",
            " Person 2:  What time do you want to leave and what time do you want to arrive by ?\n"
          ]
        }
      ],
      "source": [
        "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
        "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TFxES0zllfG"
      },
      "outputs": [],
      "source": [
        "#trích xuất các cuộc hội thoại từ tệp của tập dữ liệu.\n",
        "#Người 1: ' nếu 'x' là số chẵn hoặc ' Người 2: ' nếu 'x' là số lẻ.\n",
        "'''\n",
        "    Đầu vào:\n",
        "        file (string): tên tệp được lưu dưới dạng json\n",
        "        vd: SNG0073.json\n",
        "        data_db (dict): cơ sở dữ liệu đối thoại\n",
        "        vd: data.json\n",
        "    Returns:\n",
        "        gọi người nói tên SNG0073\n",
        "        hàm trả về toàn bộ cuộc hội thoại của người SNG0073 và trợ lý ảo\n",
        "'''\n",
        "def get_conversation(file, data_db):\n",
        "    result = ''\n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    for i in range(len_msg_log):\n",
        "        cur_log = data_db[file]['log'][i]\n",
        "        if i % 2 == 0:\n",
        "            result += delimiter_1\n",
        "        else:\n",
        "            result += delimiter_2\n",
        "        result += cur_log['text']\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnKpEK7GlzIe",
        "outputId": "25abaa96-3ffb-40e6-bb44-908a738be26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay , do you have a specific area you want to stay in ? Person 1: no , i just need to make sure it 's cheap . oh , and i need parking Person 2: I found 1 cheap hotel for you that includes parking . Do you like me to book it ? Person 1: Yes , please . 6 people 3 nights starting on tuesday . Person 2: I am sorry but I was n't able to book that for you for Tuesday . Is there another day you would like to stay or perhaps a shorter stay ? Person 1: how about only 2 nights . Person 2: Booking was successful . \n",
            " Reference number is : 7GAWK763 . Anything else I can do for you ? Person 1: No , that will be all . Good bye . Person 2: Thank you for using our services .\n"
          ]
        }
      ],
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f_hFZ-rmImC",
        "outputId": "7c3fd07f-6743-4de3-bd3c-d2e75345e9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34mPerson 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel \u001b[0m\n",
            "\u001b[31mPerson 2: Okay , do you have a specific area you want to stay in ? \u001b[0m\n",
            "\u001b[34mPerson 1: no , i just need to make sure it 's cheap . oh , and i need parking \u001b[0m\n",
            "\u001b[31mPerson 2: I found 1 cheap hotel for you that includes parking . Do you like me to book it ? \u001b[0m\n",
            "\u001b[34mPerson 1: Yes , please . 6 people 3 nights starting on tuesday . \u001b[0m\n",
            "\u001b[31mPerson 2: I am sorry but I was n't able to book that for you for Tuesday . Is there another day you would like to stay or perhaps a shorter stay ? \u001b[0m\n",
            "\u001b[34mPerson 1: how about only 2 nights . \u001b[0m\n",
            "\u001b[31mPerson 2: Booking was successful . \n",
            " Reference number is : 7GAWK763 . Anything else I can do for you ? \u001b[0m\n",
            "\u001b[34mPerson 1: No , that will be all . Good bye . \u001b[0m\n",
            "\u001b[31mPerson 2: Thank you for using our services .\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import simple_colors\n",
        "\n",
        "def print_conversation(conversation):\n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "    split_list_d1 = conversation.split(delimiter_1)\n",
        "    for sublist in split_list_d1[1:]:\n",
        "        split_list_d2 = sublist.split(delimiter_2)\n",
        "        print(simple_colors.blue(f'Person 1: {split_list_d2[0]}'))\n",
        "        if len(split_list_d2) > 1:\n",
        "            print(simple_colors.red(f'Person 2: {split_list_d2[1]}'))\n",
        "\n",
        "print_conversation(conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chúng ta sẽ chỉ sử dụng kết quả đầu ra của các lệnh gọi get_conversation để huấn luyện mô hình."
      ],
      "metadata": {
        "id": "8mT1VPFkreCk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGgCq4CnXiF6"
      },
      "source": [
        "#**Xử lý dữ liệu phù hợp với inputs của mô hình Reformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đầu vào mong muốn:\n",
        "\n",
        "Person 1: Why am I so happy? Person 2: Because you are learning NLP Person 1: ... Person 2: ...*\n",
        "\n",
        "- Person 1, Person 2 là dấu hiệu để mô hình biết ai là người đang nói để sinh văn bản phù hợp."
      ],
      "metadata": {
        "id": "U5nRDoeGrv1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmjJwxwBturC",
        "outputId": "1b3521f6-2ba6-4268-92e5-46a4de9c2bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay , do you have a specific area you want to stay in ? Person 1: no , i just need to make sure it 's cheap . oh , and i need parking Person 2: I found 1 cheap hotel for you that includes parking . Do you like me to book it ? Person 1: Yes , please . 6 people 3 nights starting on tuesday . Person 2: I am sorry but I was n't able to book that for you for Tuesday . Is there another day you would like to stay or perhaps a shorter stay ? Person 1: how about only 2 nights . Person 2: Booking was successful . \n",
            " Reference number is : 7GAWK763 . Anything else I can do for you ? Person 1: No , that will be all . Good bye . Person 2: Thank you for using our services .\n"
          ]
        }
      ],
      "source": [
        "#lấy tất cả các chuỗi hội thoại từ tất cả các tệp hội thoại và đặt chúng vào một danh sách.\n",
        "all_files = DIALOGUE_DB.keys()\n",
        "untokenized_data = []\n",
        "for file in all_files:\n",
        "    result = get_conversation(file, DIALOGUE_DB)\n",
        "    untokenized_data.append(result)\n",
        "print(untokenized_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwDjcwYesY3j",
        "outputId": "a84b35fd-a5e0-4787-d7ff-e7d989852343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng cuộc hội thoại: 10438\n",
            "Số lượng cuộc hội thoại trong tập train: 9917\n",
            "Số lượng cuộc hội thoại trong tập validation: 521\n"
          ]
        }
      ],
      "source": [
        "#Chia tập train = 95%, validation = 5%\n",
        "random.shuffle(untokenized_data)\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'Số lượng cuộc hội thoại: {len(untokenized_data)}')\n",
        "print(f'Số lượng cuộc hội thoại trong tập train: {len(train_data)}')\n",
        "print(f'Số lượng cuộc hội thoại trong tập validation: {len(eval_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenizing, batching with bucketing**"
      ],
      "metadata": {
        "id": "JPzxQ8rOtFq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9-dAW2buBR0"
      },
      "outputs": [],
      "source": [
        "def stream(data):\n",
        "    while True:\n",
        "        #chọn ngẫu nhiên data\n",
        "        d = random.choice(data)\n",
        "        # mang lại một tuple có hai giá trị giống hệt nhau\n",
        "        # đầu vào của mô hình cũng sẽ là mục tiêu của chúng tôi trong quá trình đào tạo.\n",
        "        yield (d, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg6_NGHKXri3"
      },
      "outputs": [],
      "source": [
        "#Pipeline này được sử dụng để xử lý dữ liệu đầu vào của mô hình học sâu.\n",
        "'''\n",
        "    Phân chia các mẫu trong dữ liệu thành các \"bucket\" (thùng) có kích thước gần nhau,\n",
        "    từ đó tạo ra các batch có kích thước khác nhau nhưng tối ưu.\n",
        "    Ở đây, dữ liệu được phân chia thành các bucket với ngưỡng chiều dài là [128, 256, 512, 1024],\n",
        "    và kích thước các batch được chọn tương ứng là [16, 8, 4, 2, 1].\n",
        "'''\n",
        "data_pipeline = trax.data.Serial(\n",
        "    # Sắp xếp các mẫu trong dữ liệu theo thứ tự ngẫu nhiên.\n",
        "    trax.data.Shuffle(),\n",
        "    # Chuyển đổi các mẫu trong dữ liệu từ định dạng văn bản sang định dạng số (sử dụng từ điển từ vựng được cung cấp bởi vocab_dir và vocab_file).\n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR,\n",
        "                       vocab_file=VOCAB_FILE),\n",
        "\n",
        "    # Lọc ra các mẫu trong dữ liệu có chiều dài vượt quá một giá trị ngưỡng (ở đây là 2048).\n",
        "    trax.data.FilterByLength(2048),\n",
        "    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],\n",
        "                             batch_sizes=[16,    8,    4,   2, 1]),\n",
        "    # Thêm trọng số vào các mẫu trong batch để đối phó với việc có những phần tử padding trong batch.\n",
        "    #Các phần tử padding được đánh dấu bằng giá trị 0 và không được tính trong loss.\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwdH6zBhd45_"
      },
      "outputs": [],
      "source": [
        "def ReformerLM(vocab_size=33600, n_layers=6, mode='train', attention_type=tl.SelfAttention):\n",
        "    model = tl.Serial(trax.models.reformer.ReformerLM(\n",
        "                vocab_size = vocab_size,\n",
        "                n_layers = n_layers,\n",
        "                mode = mode,\n",
        "                attention_type = attention_type\n",
        "            ), tl.LogSoftmax()\n",
        "    )\n",
        "    model.init_from_file('/content/drive/MyDrive/52000820 - Deep learning/Cuoi Ki NLP/chatbot_model1.pkl.gz', weights_only=True)\n",
        "    return model # tl.Serial(model, tl.LogSoftmax(),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1s4bm1Qd-_T"
      },
      "outputs": [],
      "source": [
        "#Hàm training_loop được sử dụng để định nghĩa một vòng lặp huấn luyện trên mô hình ReformerLM đã được tạo ra từ hàm ReformerLM.\n",
        "'''\n",
        "Đầu vào:\n",
        "ReformerLM: mô hình được sử dụng để huấn luyện.\n",
        "train_gen: dữ liệu huấn luyện.\n",
        "eval_gen: dữ liệu đánh giá.\n",
        "output_dir: đường dẫn để lưu trữ mô hình sau khi huấn luyện.\n",
        "'''\n",
        "def training_loop(ReformerLM, train_gen, eval_gen, output_dir = \"./model/\"):\n",
        "  #learning rate schedule\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
        "    '''\n",
        "    Tạo một TrainTask với các tham số:\n",
        "    dữ liệu đã được gán nhãn (labeled_data=train_gen),\n",
        "    hàm loss (loss_layer=tl.CrossEntropyLoss()),\n",
        "    optimizer (optimizer=trax.optimizers.Adam(0.01)),\n",
        "    learning rate schedule (lr_schedule=lr_schedule),\n",
        "    và số bước huấn luyện trước khi in ra thông tin (n_steps_per_checkpoint=10).\n",
        "    '''\n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=train_gen,\n",
        "        loss_layer = tl.CrossEntropyLoss(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        lr_schedule=lr_schedule,\n",
        "        n_steps_per_checkpoint=10\n",
        "    )\n",
        "    '''\n",
        "    Tạo một EvalTask với các tham số:\n",
        "    dữ liệu đã được gán nhãn (labeled_data=eval_gen)\n",
        "    và các độ đo được sử dụng để đánh giá mô hình (metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]).\n",
        "    '''\n",
        "    eval_task = training.EvalTask(labeled_data=eval_gen,\n",
        "                                  metrics=[tl.CrossEntropyLoss(), tl.Accuracy()])\n",
        "    '''\n",
        "    Tạo một Loop với các tham số:\n",
        "    mô hình (ReformerLM(mode='train')),\n",
        "    TrainTask (train_task), EvalTask (eval_task),\n",
        "    và đường dẫn đến thư mục để lưu trữ các checkpoint (output_dir=output_dir).\n",
        "    '''\n",
        "    loop = training.Loop(ReformerLM(mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    return loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZo6YuVHJsqG",
        "outputId": "f8e37280-79ab-45ac-debe-7fc69b360c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/jax/_src/xla_bridge.py:613: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 70673640\n",
            "Step      1: Ran 1 train steps in 95.53 secs\n",
            "Step      1: train CrossEntropyLoss |  2.47367144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step      1: eval  CrossEntropyLoss |  2.32425284\n",
            "Step      1: eval          Accuracy |  0.55584246\n",
            "\n",
            "Step     10: Ran 9 train steps in 352.04 secs\n",
            "Step     10: train CrossEntropyLoss |  2.17990279\n",
            "Step     10: eval  CrossEntropyLoss |  2.07066226\n",
            "Step     10: eval          Accuracy |  0.58304137\n",
            "\n",
            "Step     20: Ran 10 train steps in 245.62 secs\n",
            "Step     20: train CrossEntropyLoss |  2.14654207\n",
            "Step     20: eval  CrossEntropyLoss |  1.98243856\n",
            "Step     20: eval          Accuracy |  0.59179413\n",
            "\n",
            "Step     30: Ran 10 train steps in 239.41 secs\n",
            "Step     30: train CrossEntropyLoss |  1.90376878\n",
            "Step     30: eval  CrossEntropyLoss |  1.64894140\n",
            "Step     30: eval          Accuracy |  0.60908413\n",
            "\n",
            "Step     40: Ran 10 train steps in 247.02 secs\n",
            "Step     40: train CrossEntropyLoss |  1.77779830\n",
            "Step     40: eval  CrossEntropyLoss |  1.70110977\n",
            "Step     40: eval          Accuracy |  0.61294585\n",
            "\n",
            "Step     50: Ran 10 train steps in 278.73 secs\n",
            "Step     50: train CrossEntropyLoss |  1.71827245\n",
            "Step     50: eval  CrossEntropyLoss |  1.72000766\n",
            "Step     50: eval          Accuracy |  0.59297913\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!rm -f model/model.pkl.gz\n",
        "loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "#loop.load_checkpoint(directory='./model/', filename=\"model.pkl.gz\")\n",
        "loop.run(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEfpnHnweNOU"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSCZbPK-ePLH"
      },
      "outputs": [],
      "source": [
        "#Hàm ReformerLM_output_gen được sử dụng để tạo ra một generator cho phép dự đoán các từ tiếp theo của một câu bắt đầu với start_sentence\n",
        "#bằng cách sử dụng mô hình ReformerLM đã được huấn luyện.\n",
        "#temperature là tham số quyết định độ đa dạng của câu được tạo ra, càng cao thì kết quả sẽ càng khác nhau\n",
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature, tokenize=tokenize):\n",
        "    input_tokens = tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    input_tokens_with_batch = np.array(input_tokens)[None, :]\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream(\n",
        "        ReformerLM,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return output_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuZxmUE1uB64"
      },
      "outputs": [],
      "source": [
        "#tạo ra một tầng attention cho mạng Transformer\n",
        "#giúp kiểm soát kích thước bộ nhớ và tốc độ tính toán trong quá trình dự đoán.\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120  # xác định số lượng tokens tối đa được giữ lại trong bộ nhớ tạm thời trong quá trình dự đoán.\n",
        "    kwargs['predict_drop_len'] = 120  # chỉ định số lượng tokens tối đa được loại bỏ khi cập nhật bộ nhớ tạm thời.\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "model = ReformerLM(\n",
        "    vocab_size=33600,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMp6ap1AuEQs"
      },
      "outputs": [],
      "source": [
        "model.init_from_file('/content/model/model.pkl.gz',weights_only=True)#input_signature=shape11)\n",
        "STARTING_STATE = model.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkeMXO9seWEK"
      },
      "outputs": [],
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "    sentence = ''\n",
        "    counter = 0\n",
        "    result = [tokenize(':', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    ReformerLM.state = model_state\n",
        "    output = ReformerLM_output_gen(model, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    for o in output:\n",
        "        print(o.shape)\n",
        "        result.append(o)\n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            return (f'{sentence}')\n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            return (f'{sentence}')\n",
        "        counter += 1\n",
        "        if counter > max_len:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenation_string(sample_sentence):\n",
        "  replaced_chat = sample_sentence.replace('Person 1: ', '')\n",
        "  replaced_chat = sample_sentence.replace('Person 2: ', '')\n",
        "  replaced_chat = sample_sentence.replace(': ', '')\n",
        "  return replaced_chat"
      ],
      "metadata": {
        "id": "FpYzculwC-7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(simple_colors.red('If you want to end the conversation, enter the number 1 '))\n",
        "n = input()\n",
        "prev_chat = ''\n",
        "while n != '1':\n",
        "  print(simple_colors.blue(\"Please enter your question: \"))\n",
        "  question = input()\n",
        "  sample_sentence = \"Person 1: \" + prev_chat + question + \" Person 2: \" #nối chuỗi rồi xem nó như đầu vào để đưa vào dự đoán\n",
        "  answer = generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)\n",
        "  print(concatenation_string(answer))\n",
        "  prev_chat = concatenation_string(sample_sentence + \" \" + answer)\n",
        "  print(simple_colors.red('If you want to end the conversation, enter the number 1 '))\n",
        "  n = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG9TlLsDDRYn",
        "outputId": "9ded78da-4302-47f8-d2f0-081b76aaf6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mIf you want to end the conversation, enter the number 1 \u001b[0m\n",
            "2\n",
            "\u001b[34mPlease enter your question: \u001b[0m\n",
            "I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\n",
            ":When would you like to arrive by? \n",
            "\u001b[31mIf you want to end the conversation, enter the number 1 \u001b[0m\n",
            "2\n",
            "\u001b[34mPlease enter your question: \u001b[0m\n",
            "2\n",
            ":I've got a grey Audi for you. The contact number is 07760322670. \n",
            "\u001b[31mIf you want to end the conversation, enter the number 1 \u001b[0m\n",
            "2\n",
            "\u001b[34mPlease enter your question: \u001b[0m\n",
            "nice, thanks\n",
            ":Do you need any further assistance? \n",
            "\u001b[31mIf you want to end the conversation, enter the number 1 \u001b[0m\n",
            "2\n",
            "\u001b[34mPlease enter your question: \u001b[0m\n",
            "I'll call you when I need you\n",
            ":No problem. Have a good day!Skoda! Person i i i want to go lovell i. i want to know. i need to know the postcode, and area. \n",
            "\u001b[31mIf you want to end the conversation, enter the number 1 \u001b[0m\n",
            "1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mvyhGQvukezK",
        "vD1JQYNKkmc4"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}